{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/cc-logo-square.png\" width=\"200\" alt=\"cognitiveclass.ai logo\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Preprocessing with Principle Component Analysis </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Table of Contents</h2>\n",
    "<p>This notebook contains neural networks trained to predict house prices in King County, Washington, USA based on features of the house. They are trained using different preprocessing methods like PCA and Standardization. They are also trained using different Neural Networks with one featuring batch normalization. We will explore the benefits of data preprocessing on our neural networks and how they affect performance.</p>\n",
    "\n",
    "<ul>\n",
    "    <li><a href=\"#UFI\">Utility Functions and Imports </a></li>\n",
    "    <li><a href=\"#MDC\">Dataset Class   </a></li>\n",
    "    <li><a href=\"#DNN\"> Different Neural Networks and Function to Train Neural Network   </a></li>\n",
    "    <li><a href=\"#tv\">Training and Validate Model </a></li>\n",
    "</ul>\n",
    "\n",
    "<p>Estimated Time Needed: <strong>20 min</strong></p>\n",
    "\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"UFI\">Utility Functions and Imports </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pylab as plt\n",
    "torch.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we load the dataset we will use and the already trained models to save time instead of training from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://s3.us.cloud-object-storage.appdomain.cloud/cf-courses-data/CognitiveClass/meet_up/12.02.2020/king_county_house_data.csv\n",
    "!wget https://s3.us.cloud-object-storage.appdomain.cloud/cf-courses-data/CognitiveClass/meet_up/12.02.2020/pca_model.pt\n",
    "!wget https://s3.us.cloud-object-storage.appdomain.cloud/cf-courses-data/CognitiveClass/meet_up/12.02.2020/regular_model.pt\n",
    "!wget https://s3.us.cloud-object-storage.appdomain.cloud/cf-courses-data/CognitiveClass/meet_up/12.02.2020/standardized_model.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"DSC\">Dataset Class </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turn the csv into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('king_county_house_data.csv')\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a PyTorch Dataset object to store our dataframe values in. We create 3 type of data one for data with PCA, Standardization, and regular data with no preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class data(Dataset):\n",
    "    \n",
    "    def __init__(self, test=False, regular=False, pca=False, standard=False):\n",
    "        \n",
    "        X = df.drop(columns='price')\n",
    "        Y = df['price']\n",
    "        # Split the data into training data and testing data using a 80/20 split\n",
    "        x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=.2, random_state=1)\n",
    "        \n",
    "        x_train = x_train.astype(float)\n",
    "        x_test = x_test.astype(float)\n",
    "        \n",
    "        if (regular):\n",
    "            \n",
    "            if (test):\n",
    "                self.x = torch.FloatTensor(x_test.values)\n",
    "                self.y = torch.FloatTensor(y_test.values).view(-1,1)\n",
    "            else:\n",
    "                self.x = torch.FloatTensor(x_train.values)\n",
    "                self.y = torch.FloatTensor(y_train.values).view(-1,1)\n",
    "                \n",
    "        elif (pca):\n",
    "            \n",
    "            # Performs PCA on the data\n",
    "            pca = PCA(whiten=True)\n",
    "            x_train = pca.fit_transform(x_train)\n",
    "            x_test = pca.transform(x_test)\n",
    "            \n",
    "            if (test):\n",
    "                self.x = torch.FloatTensor(x_test)\n",
    "                self.y = torch.FloatTensor(y_test.values).view(-1,1)\n",
    "            else:\n",
    "                self.x = torch.FloatTensor(x_train)\n",
    "                self.y = torch.FloatTensor(y_train.values).view(-1,1)\n",
    "                \n",
    "        else:\n",
    "            \n",
    "            # Performs Standardization on the data\n",
    "            standard = preprocessing.StandardScaler()\n",
    "            x_train = standard.fit_transform(x_train)\n",
    "            x_test = standard.transform(x_test)\n",
    "            \n",
    "            if (test):\n",
    "                self.x = torch.FloatTensor(x_test)\n",
    "                self.y = torch.FloatTensor(y_test.values).view(-1,1)\n",
    "            else:\n",
    "                self.x = torch.FloatTensor(x_train)\n",
    "                self.y = torch.FloatTensor(y_train.values).view(-1,1)\n",
    "\n",
    "    # Getter\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"DNN\">Different Neural Networks and Function to Train Neural Network    </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define the Neural Networks we will use. They are both 4 layered networks with 1 input layer, 1 output layer, and 2 hidden layers. THe number of nodes in each layer is 2/3 of the input layer except the output because we are doing regression. They both use relu as the activation function.\n",
    "\n",
    "The difference is that one uses Batch Normalization and one does not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNormalizationNN(nn.Module):\n",
    "    \n",
    "    # Constructor\n",
    "    def __init__(self):\n",
    "        super(BatchNormalizationNN, self).__init__()\n",
    "        # The dataset has 18 features\n",
    "        self.linear1 = nn.Linear(18, 12)\n",
    "        self.b1 = nn.BatchNorm1d(12)\n",
    "        self.linear2 = nn.Linear(12, 8)\n",
    "        self.b2 = nn.BatchNorm1d(8)\n",
    "        self.linear3 = nn.Linear(8, 1)\n",
    "\n",
    "    # Prediction    \n",
    "    def forward(self, x): \n",
    "        y = self.b1(torch.relu(self.linear1(x)))\n",
    "        y = self.b2(torch.relu(self.linear2(y)))\n",
    "        y = self.linear3(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegularNN(nn.Module):\n",
    "    \n",
    "    # Constructor\n",
    "    def __init__(self):\n",
    "        super(RegularNN, self).__init__()\n",
    "        # The dataset has 18 features\n",
    "        self.linear1 = nn.Linear(18, 12)\n",
    "        self.linear2 = nn.Linear(12, 8)\n",
    "        self.linear3 = nn.Linear(8, 1)\n",
    "\n",
    "    # Prediction    \n",
    "    def forward(self, x): \n",
    "        y = torch.relu(self.linear1(x)) \n",
    "        y = torch.relu(self.linear2(y))\n",
    "        y = self.linear3(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function performs the training of the models. It is given the training data, testing data, criterion (loss function), model(Neural Network), optimizer, and epochs (Number of times the model is trained on the entire dataset). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, test_loader, criterion, model, optimizer, epochs):\n",
    "    \n",
    "    train_loss = []\n",
    "    test_loss = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        print(epoch)\n",
    "        \n",
    "        total_loss = 0\n",
    "        \n",
    "        for i, (x, y) in enumerate(train_loader):\n",
    "            # Set the model in training mode so it updates values in Batch Normalization\n",
    "            model.train()\n",
    "            # Clears the optimizer\n",
    "            optimizer.zero_grad()\n",
    "            # Calculates the prediction the model gives\n",
    "            z = model(x)\n",
    "            # Calculates the loss between the prediction and the actual value\n",
    "            loss = criterion(z, y)\n",
    "            # Calcualates the partial deriviative for each parameter of the Neural Network\n",
    "            loss.backward()\n",
    "            # Adjusts the parameters according to the optimizer\n",
    "            optimizer.step()\n",
    "            total_loss = total_loss + loss.item()\n",
    "        \n",
    "        train_loss.append(total_loss/(len(train_loader.dataset)/train_loader.batch_size))\n",
    "        print(total_loss/(len(train_loader.dataset)/train_loader.batch_size))\n",
    "        \n",
    "        total_loss = 0\n",
    "        \n",
    "        for i, (x, y) in enumerate(test_loader):\n",
    "            # Set the model in evaluation mode so we do not train the Batch Normalization while evaluating our model\n",
    "            model.eval()\n",
    "            z = model(x)\n",
    "            loss = criterion(z, y)\n",
    "            total_loss = total_loss + loss.item()\n",
    "           \n",
    "        test_loss.append(total_loss/len(test_loader.dataset))\n",
    "        print(total_loss/len(test_loader.dataset))\n",
    "        \n",
    "    return train_loss, test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"UFI\">Training and Validate Model  </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have trained the model prior you can load them and use them to run you model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vanilla Neural network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RegularModel = RegularNN()\n",
    "checkpoint = torch.load('regular_model.pt')\n",
    "RegularModel.load_state_dict(checkpoint['model_state_dict'])\n",
    "RegularModel.eval()\n",
    "regular_train_loss = checkpoint['train_loss']\n",
    "regular_test_loss = checkpoint['test_loss']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural network with Batch Norm  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCAModel = BatchNormalizationNN()\n",
    "checkpoint = torch.load('pca_model.pt')\n",
    "PCAModel.load_state_dict(checkpoint['model_state_dict'])\n",
    "PCAModel.eval()\n",
    "pca_train_loss = checkpoint['train_loss']\n",
    "pca_test_loss = checkpoint['test_loss']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural network with Batch Norm  and Starderdizing the data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "StandardModel = BatchNormalizationNN()\n",
    "checkpoint = torch.load('standardized_model.pt')\n",
    "StandardModel.load_state_dict(checkpoint['model_state_dict'])\n",
    "StandardModel.eval()\n",
    "stan_train_loss = checkpoint['train_loss']\n",
    "stan_test_loss = checkpoint['test_loss']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in the sections below is code to train the models with each type of preprocessing. The training code is saved a raw so it does not accidentally get run as it would take a lot of time to train. The loss function used is SmoothL1Loss and in its current setup it gives the absolute value of the error between the prediction and actual value. The optimizer used is Adadelta, we use this because we do not need to worry about initializing or updating the rate at which the Neural Network learns. If you would like to experiment with the Neural Networks please change the type of the training block to code using the options at the top. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular Neural Network       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you would like to train the model yourself. you can convert the following markdown cell to a code.   "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "d_train = data(regular=True, test=False)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=d_train, batch_size=500)\n",
    "d_test = data(regular=True, test=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=d_test)\n",
    "criterion = nn.SmoothL1Loss()\n",
    "RegularModel = RegularNN()\n",
    "optimizer = torch.optim.Adadelta(RegularModel.parameters())\n",
    "regular_train_loss, regular_test_loss = train(train_loader, test_loader, criterion, RegularModel, optimizer, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(regular_train_loss)\n",
    "plt.title(\"Regular Train\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Average Loss \")\n",
    "plt.show()\n",
    "print(regular_train_loss[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(regular_test_loss)\n",
    "plt.title(\"Regular Validation Data \")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Average Loss \")\n",
    "plt.show()\n",
    "print(regular_test_loss[-1])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "torch.save({\n",
    "    'epochs': 500,\n",
    "    'model_state_dict': RegularModel.state_dict(),\n",
    "    'train_loss': regular_train_loss,\n",
    "    'test_loss': regular_test_loss\n",
    "}, 'regular_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA and  Regular Neural Network  "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "d_train = data(pca=True, test=False)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=d_train, batch_size=500)\n",
    "d_test = data(pca=True, test=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=d_test)\n",
    "criterion = nn.SmoothL1Loss()\n",
    "PCAModel = BatchNormalizationNN()\n",
    "optimizer = torch.optim.Adadelta(PCAModel.parameters())\n",
    "pca_train_loss, pca_test_loss = train(train_loader, test_loader, criterion, PCAModel, optimizer, 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pca_train_loss)\n",
    "plt.title(\"PCA Train\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Average Loss\")\n",
    "plt.show()\n",
    "print(pca_train_loss[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pca_test_loss)\n",
    "plt.title(\"PCA Validation Data\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Average Loss\")\n",
    "plt.show()\n",
    "print(pca_test_loss[-1])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "torch.save({\n",
    "    'epochs': 60,\n",
    "    'model_state_dict': PCAModel.state_dict(),\n",
    "    'train_loss': pca_train_loss,\n",
    "    'test_loss': pca_test_loss\n",
    "}, 'pca_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardized Data and Neural Network     "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "d_train = data(standard=True, test=False)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=d_train, batch_size=500)\n",
    "d_test = data(standard=True, test=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=d_test)\n",
    "criterion = nn.SmoothL1Loss()\n",
    "StandardModel = BatchNormalizationNN()\n",
    "optimizer = torch.optim.Adadelta(StandardModel.parameters())\n",
    "stan_train_loss, stan_test_loss = train(train_loader, test_loader, criterion, StandardModel, optimizer, 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(stan_train_loss)\n",
    "plt.title(\"Standardization Train\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Average Loss\")\n",
    "plt.show()\n",
    "print(stan_train_loss[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(stan_test_loss)\n",
    "plt.title(\"Standardization Validation Data \")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Average Loss\")\n",
    "plt.show()\n",
    "print(stan_test_loss[-1])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "torch.save({\n",
    "    'epochs': 250,\n",
    "    'model_state_dict': StandardModel.state_dict(),\n",
    "    'train_loss': stan_train_loss,\n",
    "    'test_loss': stan_test_loss\n",
    "}, 'standardized_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viewing the Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below will allow you to see the predicted price, actual price, and error for each house in the test dataset.We will use the absolute Loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.SmoothL1Loss()\n",
    "limit = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_test = data(regular=True, test=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=d_test)\n",
    "\n",
    "for i, (x, y) in enumerate(test_loader):\n",
    "    RegularModel.eval()\n",
    "    z = RegularModel(x)\n",
    "    loss = criterion(z, y)\n",
    "    print(i)\n",
    "    print('Predicted: ',z.item())\n",
    "    print('Actual: ', y.item())\n",
    "    print('Loss: ', loss.item())\n",
    "    if (i == limit):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_test = data(pca=True, test=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=d_test)\n",
    "\n",
    "for i, (x, y) in enumerate(test_loader):\n",
    "    PCAModel.eval()\n",
    "    z = PCAModel(x)\n",
    "    loss = criterion(z, y)\n",
    "    print(i)\n",
    "    print('Predicted: ',z.item())\n",
    "    print('Actual: ', y.item())\n",
    "    print('Loss: ', loss.item())\n",
    "    if (i == limit):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_test = data(standard=True, test=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=d_test)\n",
    "\n",
    "for i, (x, y) in enumerate(test_loader):\n",
    "    StandardModel.eval()\n",
    "    z = StandardModel(x)\n",
    "    loss = criterion(z, y)\n",
    "    print(i)\n",
    "    print('Predicted: ',z.item())\n",
    "    print('Actual: ', y.item())\n",
    "    print('Loss: ', loss.item())\n",
    "    if (i == limit):\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>About the Authors:</h2> \n",
    "<p><a>Azim Hirjani</a></p>\n",
    "<p><a href=\"https://www.linkedin.com/in/joseph-s-50398b136/\">Joseph Santarcangelo</a> has a PhD in Electrical Engineering, his research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. Joseph has been working for IBM since he completed his PhD.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright &copy; 2020 <a href=\"cognitiveclass.ai?utm_source=bducopyrightlink&utm_medium=dswb&utm_campaign=bdu\">cognitiveclass.ai</a>. This notebook and its source code are released under the terms of the <a href=\"https://bigdatauniversity.com/mit-license/\">MIT License</a>."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
