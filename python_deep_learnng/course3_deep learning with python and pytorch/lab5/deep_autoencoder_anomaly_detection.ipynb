{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://cocl.us/pytorch_link_top\">\n",
    "    <img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/Pytochtop.png\" width=\"750\" alt=\"IBM Product \" />\n",
    "</a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/cc-logo-square.png\" width=\"200\" alt=\"cognitiveclass.ai logo\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Deep Autoencoder for Anomaly Detection   </h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Table of Contents</h2>\n",
    "\n",
    "<p>In this lab, we will use deep autoencoders for Anomaly detection. </p>\n",
    "\n",
    "<ul>\n",
    "    <li><a href=\"##IUF\">Imports and Utility Functions </a></li>\n",
    "<li><a href=\"#Makeup_Data\">Load Data</a></li>\n",
    "<li><a href=\"#AE\">Autoencoder</a></li>\n",
    "<li><a href=\"#Train\">Define Criterion function, Optimizer and Train the Model</a></li>\n",
    "<li><a href=\"#Result\">Analyze Results</a></li>\n",
    "</ul>\n",
    "<p>Estimated Time Needed: <strong>25 min</strong></p>\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--Empty Space for separating topics-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"IUF\">Imports and Utility Functions</h2> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the libraries we need to use in this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install Pillow==6.2.2\n",
    "\n",
    "# Using the following line code to install the torchvision library\n",
    "# !conda install -y torchvision\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "import copy\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.colors as colors\n",
    "torch.manual_seed(7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we download the  model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/models%20/autoencoders/autoencoder_anomaly.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in this function, we will plot the image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_data(data_sample, y=None):\n",
    "    plt.imshow(data_sample[0].detach().numpy().reshape(IMAGE_SIZE, IMAGE_SIZE), cmap='gray')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will plot the training and validation cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_train_val(cost_list,accuracy_list,val_data_label ='Validation error '):\n",
    "\n",
    "    fig, ax1 = plt.subplots()\n",
    "    color = 'tab:red'\n",
    "    ax1.plot(cost_list, color = color)\n",
    "    ax1.set_xlabel('epoch ', color = color)\n",
    "    ax1.set_ylabel('total loss', color = color)\n",
    "    ax1.tick_params(axis = 'y', color = color)\n",
    "\n",
    "    ax2 = ax1.twinx()  \n",
    "    color = 'tab:blue'\n",
    "    ax2.set_ylabel(val_data_label, color = color)  # we already handled the x-label with ax1\n",
    "    ax2.plot(accuracy_list, color = color)\n",
    "    ax2.tick_params(axis = 'y', color = color)\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"Makeup_Data\">Load the Data</h2> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we create a transform to resize the image and convert it to a tensor :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 16\n",
    "tensor_size=IMAGE_SIZE*IMAGE_SIZE\n",
    "composed = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)), transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the training dataset by setting the parameters <code>train </code> to <code>True</code>. We use the transform defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_dataset = dsets.FashionMNIST(root='./data', train=True, download=True, transform=composed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the testing dataset by setting the parameters train  <code>False</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make the validating \n",
    "\n",
    "validation_dataset = dsets.FashionMNIST(root='./data', train=False, download=True, transform=composed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the data type is long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Show the data type for each element in dataset\n",
    "train_dataset[0][1].type()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each element in the rectangular tensor corresponds to a number representing a pixel intensity as demonstrated by the following image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/chapter%206/6.2.1imagenet.png\" width=\"550\" alt=\"MNIST data image\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out the fourth label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The label for the fourth data element\n",
    "\n",
    "train_dataset[3][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_std=0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the fourth sample "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The image for the fourth data element\n",
    "show_data(train_dataset[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create a train loader and data loader  object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_batch_size=100\n",
    "validation_batch_size=5000\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=train_batch_size)\n",
    "validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=validation_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--Empty Space for separating topics-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"AE\">AutoEncoder</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we build an autoencoder class or custom module with one layer. We also Build a function to train it using the mean square error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoderone_hidden(nn.Module):\n",
    "    def __init__(self, input_dim=2,encoding_dim_1=2,encoding_dim_2=2):\n",
    "        super(Autoencoderone_hidden,self).__init__()\n",
    "      \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, encoding_dim_1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(encoding_dim_1, encoding_dim_2),nn.ReLU())\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(encoding_dim_2, encoding_dim_1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(encoding_dim_1, input_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        \n",
    "        return x\n",
    "    def code(self,x):\n",
    "        return self.encoder(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method trains the autoencoder; the parameter <code>model</code> is the autoencoder object. The parameter  <code>train_loader</code> and <code>validation_loader</code> is the train loader and validation loader.  The Parameter optimizer is the optimizer object, and <code>n_epoch</code> is the number of epochs  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,train_loader,validation_loader,optimizer,n_epochs=4,checkpoint_path=None,checkpoint=None):   \n",
    "    #global variable \n",
    "    cost_list_training =[]\n",
    "    cost_list_validation =[]\n",
    "    for epoch in range(n_epochs):\n",
    "        cost_training=0\n",
    "        for x, y in train_loader:\n",
    "           \n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            z = model(x.view(-1,256))\n",
    "            loss = criterion(z, x.view(-1,256))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            cost_training+=loss.data\n",
    "        \n",
    "        cost_list_training.append(cost_training)\n",
    "    \n",
    "       \n",
    "        print(\"epoch {}, Cost {}\".format(epoch+1,cost_training) )\n",
    "        #perform a prediction on the validation  data  \n",
    "        cost_val=0\n",
    "        for x_test, y_test in validation_loader:\n",
    "            \n",
    "            model.eval()\n",
    "            \n",
    "            z = model(x.view(-1,256))\n",
    "            loss = criterion(z, x.view(-1,256))\n",
    "            cost_val+=loss.data\n",
    "            \n",
    "            \n",
    "        \n",
    "        cost_list_validation.append(cost_val)\n",
    "        \n",
    "        if checkpoint:\n",
    "            checkpoint['epoch']=epoch\n",
    "            checkpoint['model_state_dict']=model.state_dict()\n",
    "            checkpoint['optimizer_state_dict']= optimizer.state_dict()\n",
    "            checkpoint['loss']=loss \n",
    "            checkpoint['training_cost']=cost_list_training\n",
    "            checkpoint['validaion_cost']=cost_list_validation\n",
    "            torch.save(checkpoint, checkpoint_path)\n",
    "     \n",
    "    return cost_list_training, cost_list_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"Train\"> Visualizing the Code</h2> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create an autoencoder object, criterion function and optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= Autoencoderone_hidden(input_dim=tensor_size ,encoding_dim_1=400,encoding_dim_2=100)\n",
    "criterion = nn.MSELoss()\n",
    "learning_rate = 0.0001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment to train the model yourself; otherwise, you can load the model on the next line of code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "checkpoint={'epoch':None,'model_state_dict':None ,'optimizer_state_dict':None ,'loss': None ,'training_cost':None,'validaion_cost':None }\n",
    "checkpoint_path='autoencoder_anomaly.pt'\n",
    "cost_list_training, cost_list_validation=train_model(model,train_loader,validation_loader,optimizer,n_epochs=10,checkpoint_path=checkpoint_path,checkpoint=checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we load the model and checkpoints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path='autoencoder_anomaly.pt'\n",
    "checkpoint= torch.load(checkpoint_path)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load and plot the cost for the training and validation data:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_list_training, cost_list_validation=checkpoint['training_cost'], checkpoint['validaion_cost']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_train_val(cost_list_training, cost_list_validation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then calculate the loss for each sample in the validation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_error=[nn.MSELoss()(x.squeeze().view(-1,256), model(x.squeeze().view(-1,256))).item() for x,_ in validation_dataset ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The samples with the 10 larget loss are given  by: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "largest=np.argsort(val_error)[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we plot out the samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in largest:\n",
    "    show_data(validation_dataset[l][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://cocl.us/pytorch_link_bottom\">\n",
    "    <img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/notebook_bottom%20.png\" width=\"750\" alt=\"PyTorch Bottom\" />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>About the Authors:</h2> \n",
    "\n",
    "<a href=\"https://www.linkedin.com/in/joseph-s-50398b136/\">Joseph Santarcangelo</a> has a PhD in Electrical Engineering, his research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. Joseph has been working for IBM since he completed his PhD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright &copy; 2018 <a href=\"cognitiveclass.ai?utm_source=bducopyrightlink&utm_medium=dswb&utm_campaign=bdu\">cognitiveclass.ai</a>. This notebook and its source code are released under the terms of the <a href=\"https://bigdatauniversity.com/mit-license/\">MIT License</a>."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
