{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://cocl.us/pytorch_link_top\">\n",
    "    <img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/Pytochtop.png\" width=\"750\" alt=\"IBM Product \" />\n",
    "</a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/cc-logo-square.png\" width=\"200\" alt=\"cognitiveclass.ai logo\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Principal Component Analysis (PCA) and Autoencoders </h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Table of Contents</h2>\n",
    "<p>In this lab, we will compare Principal Component Analysis (PCA), we will show how it can reduce the dimensions of the data..  </p>\n",
    "\n",
    "<ul>\n",
    "    <li><a href=\"#prep\">Preparation</a></li>\n",
    "    <li><a href=\"#PCA\"> PCA 2D </a></li>\n",
    "    <li><a href=\"#A2D\">Autoencoders 2D </a></li>\n",
    "    <li><a href=\"#PCA3D\">PCA 3D</a></li>\n",
    "    <li><a href=\"#A3D\">Autoencoders 3D </a></li>\n",
    "\n",
    "</ul>\n",
    "\n",
    "<p>Estimated Time Needed: <strong>30 min</strong></p>\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"prep\">Preparation</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need the following labs and functions: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the libraries we are going to use in the lab.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "from mpl_toolkits.mplot3d import proj3d\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from matplotlib.patches import FancyArrowPatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple autoencoder module or class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module): \n",
    "  \n",
    "    def __init__(self, input_dim=2, encoding_dim=32):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        \n",
    "        self.encoder = nn.Linear(input_dim,encoding_dim)\n",
    "        self.decoder = nn.Linear(encoding_dim,input_dim)\n",
    "    \n",
    "  \n",
    "    def forward(self, x):\n",
    " \n",
    "        x=self.encoder(x)\n",
    "        x=self.decoder(x)\n",
    "      \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "function to train an autoencoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,X,optimizer,n_epochs=4):   \n",
    "    #global variable \n",
    "    cost_list_training =[]\n",
    "    cost_list_validation =[]\n",
    "    for epoch in range(n_epochs):\n",
    "        cost_training=0\n",
    "        for x in X:\n",
    "           \n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            xhat = model(x)\n",
    "            loss = criterion(xhat, x)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            cost_training+=loss.data\n",
    "        \n",
    "        cost_list_training.append(cost_training)\n",
    "     \n",
    "    return cost_list_training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will plot the data in 2D-data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(X,Xhat,vec=False,label_X=\"data\",label_Xhat=\"transformed data\",lable_vec=\"parameter vector\"):\n",
    "\n",
    "    if type(X) is torch.Tensor:\n",
    "        plt.scatter(X[:, 0].numpy(), X[:, 1].numpy(),label=label_X) \n",
    "    if type(Xhat) is torch.Tensor:   \n",
    "        plt.scatter(Xhat[:, 0].numpy(), Xhat[:, 1].numpy(),label=label_Xhat)\n",
    "    if type(vec) is torch.Tensor:\n",
    "        \n",
    "        plt.xlabel(\"q_{1}\")\n",
    "        plt.ylabel(\"q_{2}\")\n",
    "        plt.quiver([0],[0],vec[:,0],vec[:,1],label=lable_vec)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will generate 3D data from a plane and add noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenerateData(ProjectionMatrix=[[1,-1,0],[2,1,0],[0,0,0]],Noise_std=0.1,Nsamples=1000):\n",
    "    \n",
    "    import numpy as np\n",
    "    np.random.seed(0)\n",
    "\n",
    "    Max=1\n",
    "    Min=-1\n",
    "    #Create a projection matrix\n",
    "    ProjectionMatrix=np.array([[1,-1,0],[2,1,0],[0,0,0]])\n",
    "    #Acreate some noise and add to data  \n",
    "    Noise=Noise_std*np.random.normal(loc=0.0, scale=1, size=(Nsamples,3))\n",
    "    #Create some uniform data and  projection onto plane \n",
    "    TureData=np.dot(ProjectionMatrix,np.random.uniform(low=Min, high=Max, size=(Nsamples,3)).T ).T\n",
    "    TureData=TureData-TureData.mean( axis=0)\n",
    "    #add noise \n",
    "    CorruptedData =TureData+Noise\n",
    "    CorruptedData =CorruptedData -CorruptedData .mean( axis=0) \n",
    "    return TureData,CorruptedData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will plot the 3D data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PlotData3D(Data,Data1,Name1=\"Data 1\",Name2=\"Data 2\",BasisVectors=None):\n",
    "    fig = plt.figure(figsize=(15,15))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.plot(Data[:,0], Data[:,1], Data[:,2], 'x', markersize=10, color='b', alpha=1,label=Name1)\n",
    "    ax.plot(Data1[:,0], Data1[:,1], Data1[:,2], 'o', markersize=10, color='r', alpha=0.5,label=Name2)\n",
    "    \n",
    "    if type(BasisVectors)==torch.Tensor:\n",
    "        for q in torch.transpose(BasisVectors,0,1):\n",
    "            print(q)\n",
    "            a = Arrow3D([0,q[0].numpy()], [0, q[1].numpy()], [0, q[2].numpy()], mutation_scale=20, lw=3, arrowstyle=\"-|>\", color=\"k\")\n",
    "            ax.add_artist(a)\n",
    "  \n",
    "    ax.set_zlim([-2,2])\n",
    "    ax.set_xlim([-2,2])\n",
    "    ax.set_ylim([-2,2])\n",
    "    ax.legend(shadow=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will create the arrows in 3D to represent vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arrow3D(FancyArrowPatch):\n",
    "    #code by CT Zhu\n",
    "    #https://stackoverflow.com/users/2487184/ct-zhu\n",
    "    def __init__(self, xs, ys, zs, *args, **kwargs):\n",
    "        FancyArrowPatch.__init__(self, (0,0), (0,0), *args, **kwargs)\n",
    "        self._verts3d = xs, ys, zs\n",
    "\n",
    "    def draw(self, renderer):\n",
    "        xs3d, ys3d, zs3d = self._verts3d\n",
    "        xs, ys, zs = proj3d.proj_transform(xs3d, ys3d, zs3d, renderer.M)\n",
    "        self.set_positions((xs[0],ys[0]),(xs[1],ys[1]))\n",
    "        FancyArrowPatch.draw(self, renderer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"#PCA \"> PCA 2D</h2>\n",
    "In this section, we create a dataset object that uses Principal component analysis (PCA). We find the projection of the data on the eigenvectors of the covariance matrix $\\mathbf{Q}$, as shown below. We zero center the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate 2D  data that is correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples=200\n",
    "u=torch.tensor([[1.0,1.0],[0.10,-0.10]])/(2)**(0.5)\n",
    "X=torch.mm(4*torch.randn(samples,2),u)+2\n",
    "plt.scatter(X[:, 0].numpy(), X[:, 1].numpy())\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From now on we will deal with zero zero centered data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathbf x=\\mathbf x-\\boldsymbol\\mu$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_old=X\n",
    "X=X-X.mean(dim=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the date before we subtract and after we subtract the mean.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(X_old,X,label_X=\"original data\",label_Xhat=\"zero-mean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{1}{N}   \\mathbf{X}^T \\mathbf{X} = \\mathbf{Q} \\mathbf{\\Lambda} \\mathbf{Q}^T$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculate the empirical covariance matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{1}{N}   \\mathbf{X}^T \\mathbf{X}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cov=torch.mm(torch.t(X),X)/X.shape[0]\n",
    "Cov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain the eigenvectors\n",
    "$\\frac{1}{N}   \\mathbf{X}^T \\mathbf{X} = \\mathbf{Q} \\mathbf{\\Lambda} \\mathbf{Q}^T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvalues,eigenvectors=torch.eig(Cov,True)\n",
    "eigenvectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we have the eigenvalues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can plot the eigenvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_vec=torch.t(eigenvectors).numpy()\n",
    "plt.scatter(X[:, 0].numpy(), X[:, 1].numpy(),label=\"data\")\n",
    "plt.quiver([0],[0],row_vec[:,0],row_vec[:,1],label=\"Eigen vectors\")\n",
    "plt.xlabel(\"x_{1}\")\n",
    "plt.ylabel(\"x_{2}\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find the projection  the eigenvectors:\n",
    "$Z= \\mathbf{X} \\mathbf{Q} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z=torch.mm(X,eigenvectors)\n",
    "plot_data(X,Z,eigenvectors,label_X=\"data\",label_Xhat=\"Z\",lable_vec=\"q\")\n",
    "torch.mm(torch.transpose(Z,0,1),Z)/Z.size()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We can also find the $\\mathbf{\\hat{X}}$, as we are using all the vectors, we have a perfect reconstruction. \n",
    "$\\mathbf{\\hat{X}}=\\mathbf{Q} \\mathbf{Z} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xhat=torch.mm(Z,torch.transpose(eigenvectors,0,1))\n",
    "plot_data(X,Xhat,eigenvectors,label_X=\"X\",label_Xhat=\"Xhat\",lable_vec=\"parameter vector\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also find the max eigenvalue  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_eigenvalue=torch.argmax(eigenvalues[:,0])\n",
    "max_eigenvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we find the corresponding max eigenvector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_eigenvector=eigenvectors[:,max_eigenvalue]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To keep the tensors the same shape we simply copy the eigenvector to a new tensor. This behaves like setting the other eigenvector to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_Q=torch.zeros(2,2)\n",
    "new_Q[:,0]=max_eigenvector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find Xhat, this time as it’s an approximation all the points fall on the line that is a scaler multiple of are eigenvector.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z=torch.mm(X,new_Q)\n",
    "Xhat=torch.mm(Z,torch.transpose(new_Q,0,1))\n",
    "plot_data(X,Xhat,eigenvectors,label_X=\"data\",label_Xhat=\"Xhat\",lable_vec=\"parameter vector\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"#A2D<\">Autoencoders 2D </h2> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do the same for an autoencoder, but instead of only selecting one eigenvector we set the code length to be one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=AutoEncoder(2,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like a neural network we train the encoder using gradient descent.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "cost_list_training=train_model(model,X,optimizer,n_epochs=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make a prediction for <code>Xhat</code>  and plot it.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xhat=model(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(X,model(X).detach(),eigenvectors,label_X=\"data\",label_Xhat=\"Xhat\",lable_vec=\"parameter vector\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the output is similar to  PCA. The data spans a line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"#PCA3D<\"> PCA 3D</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Let’s do the same for three dimensional points  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathbf{\\hat{X}}=\\mathbf{X} \\mathbf{Q} \\mathbf{\\Lambda}^{-1/2} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s do the same for three dimensional points. We will generate  some data from a plane and add noise. We will convert it to a PyTorch tensor and subtract the mean. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TureData,CorruptedData=GenerateData(ProjectionMatrix=[[1,-1,0],[2,1,0],[0,0,0]],Noise_std=0.1,Nsamples=1000)\n",
    "TureData\n",
    "X=torch.from_numpy(CorruptedData).float()\n",
    "X=X-X.mean(dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot out the original points in blue and overlay the points with added noise in red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PlotData3D(TureData,CorruptedData,Name1=\"Orignal Data \",Name2=\"Corrupted Data \",BasisVectors=torch.eye(3))\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can determine covariance matrix and it  eigenvalues and eigenvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cov=torch.mm(torch.t(X),X)/X.shape[0]\n",
    "eigenvalues,eigenvectors=torch.eig(Cov,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can verify  Orthogonal i.e parallel, more specifically orthonormal basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mm(torch.transpose(eigenvectors,0,1),eigenvectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find the index of the first two largest eigenvectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexs=set([0,1,2] )\n",
    "min_eigenvalues=torch.argmin(eigenvalues[:,0]).item()\n",
    "indexs.remove(min_eigenvalues)\n",
    "indexs=list(indexs)\n",
    "eigenvectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To keep the tensors the same shape we simply copy the top eigenvector to a new tensor. This behaves like setting the other eigenvector to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_Q=torch.zeros(3,3)\n",
    "for i,index in enumerate(indexs):\n",
    "    new_Q[:,i]=eigenvectors[:,index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find the code and the prediction  $\\mathbf{\\hat{X}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z=torch.mm(X,new_Q)\n",
    "Z\n",
    "Xhat=torch.mm(Z,torch.transpose(new_Q,0,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compare the output compared to the data points with noise. We see the  output points fall in the line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PlotData3D(Xhat.numpy(),CorruptedData,Name1=\"Xhat\",Name2=\"Data with Noise\",BasisVectors=new_Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"A3D<\"> Autoencoder in 3D</h2> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can perform a similar process with an Autoencoder but the vectors are not the same. We can verify this by showing that the basis are not orthogonal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=AutoEncoder(3,3)\n",
    "learning_rate = 0.01\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "cost_list_training=train_model(model,X,optimizer,n_epochs=4)\n",
    "Xhat=model(X)\n",
    "torch.mm(torch.transpose(model.state_dict()['encoder.weight'],0,1), model.state_dict()['encoder.weight'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the autoencoder for dimensionality reduction as well, we can reduce the code to two. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=AutoEncoder(3,2)\n",
    "learning_rate = 0.01\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "cost_list_training=train_model(model,X,optimizer,n_epochs=4)\n",
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.encoder(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the output spans the same plane.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xhat=model(X)\n",
    "PlotData3D(Xhat.detach().numpy(),CorruptedData,Name1=\"Xhat\",Name2=\"Data with Noise\",BasisVectors=model.state_dict()['decoder.weight'].detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--Empty Space for separating topics-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>About the Authors:</h2> \n",
    "\n",
    "<a href=\"https://www.linkedin.com/in/joseph-s-50398b136/\">Joseph Santarcangelo</a> has a PhD in Electrical Engineering, his research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. Joseph has been working for IBM since he completed his PhD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright &copy; 2020 <a href=\"cognitiveclass.ai?utm_source=bducopyrightlink&utm_medium=dswb&utm_campaign=bdu\">cognitiveclass.ai</a>. This notebook and its source code are released under the terms of the <a href=\"https://bigdatauniversity.com/mit-license/\">MIT License</a>."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
