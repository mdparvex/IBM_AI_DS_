{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://cocl.us/pytorch_link_top\">\n",
    "    <img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/Pytochtop.png\" width=\"750\" alt=\"IBM Product \" />\n",
    "</a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/cc-logo-square.png\" width=\"200\" alt=\"cognitiveclass.ai logo\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Deep Autoencoders for Noise Removal  </h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Table of Contents</h2>\n",
    "\n",
    "<p>In this lab, you will train a Deep Autoencoder to  remove noise </p>\n",
    "\n",
    "<ul>\n",
    "    <li><a href=\"#IUF\">Imports and Utility Functions </a></li>\n",
    "<li><a href=\"#Makeup_Data\">Load Data</a></li>\n",
    "<li><a href=\"#DA\">Deep Autoencoder</a></li>\n",
    "<li><a href=\"#Train\">Define Criterion function, Optimizer and Train the Model</a></li>\n",
    "<li><a href=\"#R\">Results</a></li>\n",
    "</ul>\n",
    "<p>Estimated Time Needed: <strong>25 min</strong></p>\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--Empty Space for separating topics-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"IUF\">Imports and Utility Functions </h2> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the libraries we need to use in this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Pillow==6.2.2 in /Users/joseph/anaconda/lib/python3.6/site-packages (6.2.2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x109be87d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install Pillow==6.2.2\n",
    "\n",
    "# Using the following line code to install the torchvision library\n",
    "# !conda install -y torchvision\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "import copy\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.colors as colors\n",
    "torch.manual_seed(7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "download the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/models%20/autoencoders/deepauto_image_noise_removal.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "function to plot data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_data(data_sample, y=None):\n",
    "    plt.imshow(data_sample[0].detach().numpy().reshape(IMAGE_SIZE, IMAGE_SIZE), cmap='gray')\n",
    "    \n",
    "    \n",
    "   \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot training and validation data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_train_val(cost_list,accuracy_list,val_data_label ='Validation error '):\n",
    "\n",
    "    fig, ax1 = plt.subplots()\n",
    "    color = 'tab:red'\n",
    "    ax1.plot(cost_list, color = color)\n",
    "    ax1.set_xlabel('epoch ', color = color)\n",
    "    ax1.set_ylabel('total loss', color = color)\n",
    "    ax1.tick_params(axis = 'y', color = color)\n",
    "\n",
    "    ax2 = ax1.twinx()  \n",
    "    color = 'tab:blue'\n",
    "    ax2.set_ylabel(val_data_label, color = color)  # we already handled the x-label with ax1\n",
    "    ax2.plot(accuracy_list, color = color)\n",
    "    ax2.tick_params(axis = 'y', color = color)\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot images, images with noise and images after passed through the autoencoders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_autoencoder(model,dataset,noise_std,samples=[1,23,45]):\n",
    "    for sample in samples:\n",
    "        x=validation_dataset[sample][0]\n",
    "        x_=x+noise_std*torch.randn((1,16,16))\n",
    "        xhat=model(x_.view(-1,256))\n",
    "\n",
    "\n",
    "        plt.figure()\n",
    "\n",
    "        plt.subplot(131)\n",
    "        plt.imshow(x.detach().numpy().reshape(IMAGE_SIZE, IMAGE_SIZE), cmap='gray')\n",
    "        plt.title('Original image')\n",
    "\n",
    "        plt.subplot(132)\n",
    "        plt.imshow(x_.detach().numpy().reshape(IMAGE_SIZE, IMAGE_SIZE), cmap='gray')\n",
    "        plt.title('noisy image')\n",
    "\n",
    "        plt.subplot(133)\n",
    "        plt.imshow(xhat.detach().numpy().reshape(IMAGE_SIZE, IMAGE_SIZE), cmap='gray')\n",
    "  \n",
    "        plt.title('output of autoencoder')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"Makeup_Data\">Load Data</h2> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we create a transform to resize the image and convert it to a tensor :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 16\n",
    "tensor_size=IMAGE_SIZE*IMAGE_SIZE\n",
    "composed = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)), transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the training dataset by setting the parameters <code>train </code> to <code>True</code>. We use the transform defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_dataset = dsets.FashionMNIST(root='./data', train=True, download=True, transform=composed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the testing dataset by setting the parameters train  <code>False</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make the validating \n",
    "\n",
    "validation_dataset = dsets.FashionMNIST(root='./data', train=False, download=True, transform=composed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the data type is long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Show the data type for each element in dataset\n",
    "train_dataset[0][1].type()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each element in the rectangular tensor corresponds to a number representing a pixel intensity as demonstrated by the following image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/chapter%206/6.2.1imagenet.png\" width=\"550\" alt=\"MNIST data image\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out the fourth label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The label for the fourth data element\n",
    "\n",
    "train_dataset[3][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_std=0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the fourth sample "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The image for the fourth data element\n",
    "show_data(train_dataset[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create a train loader and data loader  object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_batch_size=100\n",
    "validation_batch_size=5000\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=train_batch_size)\n",
    "validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=validation_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--Empty Space for separating topics-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"DA\">Deep Autoencoder</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we build an autoencoder class or custom module with one layer. We also Build a function to train it using the mean square error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoderone_hidden(nn.Module):\n",
    "    def __init__(self, input_dim=2,encoding_dim_1=2,encoding_dim_2=2):\n",
    "        super(Autoencoderone_hidden,self).__init__()\n",
    "      \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, encoding_dim_1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(encoding_dim_1, encoding_dim_2),nn.ReLU())\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(encoding_dim_2, encoding_dim_1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(encoding_dim_1, input_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        \n",
    "        return x\n",
    "    def code(self,x):\n",
    "        return self.encoder(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method trains the autoencoder; the parameter <code>model</code> is the autoencoder object. The parameter  <code>train_loader</code> and <code>validation_loader</code> is the train loader and validation loader.  The Parameter optimizer is the optimizer object, and <code>n_epoch</code> is the number of epochs  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,train_loader,validation_loader,optimizer,n_epochs=4,noise_std=0.1, train_batch_size=100,validation_batch_size=5000,checkpoint_path=None,checkpoint=None):   \n",
    "    #global variable \n",
    "    cost_list_training =[]\n",
    "    cost_list_validation =[]\n",
    "    for epoch in range(n_epochs):\n",
    "        cost_training=0\n",
    "        for x, y in train_loader:\n",
    "           \n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            x_ =x+noise_std*torch.randn((train_batch_size,1,16,16))\n",
    "          \n",
    "            z = model(x_.view(-1,256))\n",
    "            loss = criterion(z, x.view(-1,256))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            cost_training+=loss.data\n",
    "        \n",
    "        cost_list_training.append(cost_training)\n",
    "    \n",
    "       \n",
    "        print(\"epoch {}, Cost {}\".format(epoch+1,cost_training) )\n",
    "        #perform a prediction on the validation  data  \n",
    "        cost_val=0\n",
    "        for x_test, y_test in validation_loader:\n",
    "            \n",
    "            model.eval()\n",
    "            x_ =x_test+noise_std*torch.randn((validation_batch_size,1,16,16))\n",
    "            z = model(x_.view(-1,256))\n",
    "            loss = criterion(z, x_test.view(-1,256))\n",
    "            cost_val+=loss.data\n",
    "            \n",
    "        cost_list_validation.append(cost_val) \n",
    "        \n",
    "    if checkpoint:\n",
    "        checkpoint['epoch']=epoch\n",
    "        checkpoint['model_state_dict']=model.state_dict()\n",
    "        checkpoint['optimizer_state_dict']= optimizer.state_dict()\n",
    "        checkpoint['loss']=loss \n",
    "        checkpoint['training_cost']=cost_list_training\n",
    "        checkpoint['validaion_cost']=cost_list_validation\n",
    "        torch.save(checkpoint, checkpoint_path)\n",
    "     \n",
    "    return cost_list_training, cost_list_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"R\"> Results</h2> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the model using the Root means square error to remove the noise. We create an autoencoder object "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the model, criterion and optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= Autoencoderone_hidden(input_dim=tensor_size ,encoding_dim_1=500,encoding_dim_2=100)\n",
    "criterion = nn.MSELoss()\n",
    "learning_rate = 0.0001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment to train the model yourself; otherwise, you can load the model on the next line of code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "checkpoint={'epoch':None,'model_state_dict':None ,'optimizer_state_dict':None ,'loss': None ,'training_cost':None,'validaion_cost':None }\n",
    "checkpoint_path='deepauto_image_noise_removal.pt'\n",
    "cost_list_training, cost_list_validation=train_model(model,train_loader,validation_loader,optimizer,n_epochs=20,noise_std=0.15,checkpoint_path=checkpoint_path,checkpoint=checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will load the checkpoint from memory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path='deepauto_image_noise_removal.pt'\n",
    "checkpoint= torch.load(checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load the training and validation cost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_list_training, cost_list_validation= checkpoint['training_cost'], checkpoint['validaion_cost']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "display the result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_train_val(cost_list_training, cost_list_validation)\n",
    "plot_autoencoder(model,validation_dataset,noise_std=0.2,samples=[1,23,45])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://cocl.us/pytorch_link_bottom\">\n",
    "    <img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/notebook_bottom%20.png\" width=\"750\" alt=\"PyTorch Bottom\" />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>About the Authors:</h2> \n",
    "\n",
    "<a href=\"https://www.linkedin.com/in/joseph-s-50398b136/\">Joseph Santarcangelo</a> has a PhD in Electrical Engineering, his research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. Joseph has been working for IBM since he completed his PhD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright &copy; 2020 <a href=\"cognitiveclass.ai?utm_source=bducopyrightlink&utm_medium=dswb&utm_campaign=bdu\">cognitiveclass.ai</a>. This notebook and its source code are released under the terms of the <a href=\"https://bigdatauniversity.com/mit-license/\">MIT License</a>."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
